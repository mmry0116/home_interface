import { createClass as _createClass, objectSpread2 as _objectSpread2, classCallCheck as _classCallCheck, defineProperty as _defineProperty } from "../../../_virtual/_rollupPluginBabelHelpers.js";
import { Logger, concatUint8Array, StreamingError, ERR } from "xgplayer-streaming-shared";
import { TrackType, WarningType, FMP4Demuxer, TsDemuxer, FMP4Remuxer } from "xgplayer-transmuxer";
import { Event } from "../../constants.js";
var logger = new Logger("Transmuxer");
var Transmuxer = /* @__PURE__ */ function() {
  function Transmuxer2(hls, isMP4, needRemux) {
    _classCallCheck(this, Transmuxer2);
    _defineProperty(this, "_initSegmentId", "");
    this.hls = hls;
    this._demuxer = isMP4 ? new FMP4Demuxer() : new TsDemuxer();
    this._isMP4 = isMP4;
    if (needRemux)
      this._remuxer = new FMP4Remuxer(this._demuxer.videoTrack, this._demuxer.audioTrack);
  }
  _createClass(Transmuxer2, [{
    key: "transmux",
    value: function transmux(videoChunk, audioChunk, discontinuity, contiguous, startTime, needInit) {
      var demuxer = this._demuxer;
      try {
        if (this._isMP4) {
          demuxer.demux(videoChunk, audioChunk);
        } else {
          demuxer.demuxAndFix(concatUint8Array(videoChunk, audioChunk), discontinuity, contiguous, startTime);
        }
      } catch (error) {
        throw new StreamingError(ERR.DEMUX, ERR.SUB_TYPES.HLS, error);
      }
      var videoTrack = demuxer.videoTrack, audioTrack = demuxer.audioTrack, metadataTrack = demuxer.metadataTrack;
      var vParsed = {
        codec: videoTrack.codec,
        timescale: videoTrack.timescale,
        firstDts: videoTrack.firstDts / videoTrack.timescale,
        firstPts: videoTrack.firstPts / videoTrack.timescale,
        duration: videoTrack.samplesDuration / videoTrack.timescale
      };
      var aParsed = {
        codec: audioTrack.codec,
        timescale: audioTrack.timescale,
        firstDts: audioTrack.firstDts / videoTrack.timescale,
        firstPts: audioTrack.firstPts / videoTrack.timescale,
        duration: audioTrack.samplesDuration / videoTrack.timescale
      };
      var newId = "".concat(videoTrack.codec, "/").concat(videoTrack.width, "/").concat(videoTrack.height, "/").concat(audioTrack.codec, "/").concat(audioTrack.config);
      if (newId !== this._initSegmentId) {
        this._initSegmentId = newId;
        needInit = true;
      }
      this._fireEvents(videoTrack, audioTrack, metadataTrack, discontinuity || needInit);
      this.hls.emit(Event.DEMUXED_TRACK, {
        videoTrack,
        audioTrack
      });
      if (this._remuxer) {
        if (needInit && this.hls.isLive && !this.hls.config.mseLowLatency) {
          videoTrack.duration = this.hls.totalDuration * videoTrack.timescale;
          audioTrack.duration = this.hls.totalDuration * audioTrack.timescale;
        }
        try {
          var _this$_remuxer$remux = this._remuxer.remux(needInit), videoInitSegment = _this$_remuxer$remux.videoInitSegment, videoSegment = _this$_remuxer$remux.videoSegment, audioInitSegment = _this$_remuxer$remux.audioInitSegment, audioSegment = _this$_remuxer$remux.audioSegment;
          var v = concatUint8Array(videoInitSegment, videoSegment);
          var a = concatUint8Array(audioInitSegment, audioSegment);
          return [v ? _objectSpread2(_objectSpread2({}, vParsed), {}, {
            data: v
          }) : void 0, a ? _objectSpread2(_objectSpread2({}, aParsed), {}, {
            data: a
          }) : void 0];
        } catch (error) {
          throw new StreamingError(ERR.REMUX, ERR.SUB_TYPES.FMP4, error);
        }
      } else {
        return [videoTrack, audioTrack];
      }
    }
  }, {
    key: "_fireEvents",
    value: function _fireEvents(videoTrack, audioTrack, metadataTrack, discontinuity) {
      var _this = this;
      var tracks = [videoTrack, audioTrack];
      var logCC = "discontinuity: ".concat(discontinuity);
      tracks.forEach(function(track) {
        var _track$samples;
        if ((_track$samples = track.samples) !== null && _track$samples !== void 0 && _track$samples.length) {
          logCC += "; ".concat(track.samples.length, " ").concat(track.type === TrackType.VIDEO ? "video" : "audio", " samples, firstDts/firstPts/duration: ").concat((track.firstDts / track.timescale).toFixed(3), "/").concat((track.firstPts / track.timescale).toFixed(3), "/").concat((track.samplesDuration / track.timescale).toFixed(3));
        }
        if (discontinuity && track.exist()) {
          _this.hls.emit(Event.METADATA_PARSED, {
            type: track.type,
            track,
            meta: _objectSpread2({
              codec: track.codec,
              timescale: track.timescale,
              baseDts: track.baseDts
            }, track.type === TrackType.VIDEO ? {
              width: track.width,
              height: track.height,
              sarRatio: track.sarRatio
            } : {
              codec: track.codec,
              channelCount: track.channelCount,
              sampleRate: track.sampleRate
            })
          });
        }
      });
      logger.debug(logCC);
      videoTrack.warnings.forEach(function(warn) {
        var type;
        switch (warn.type) {
          case WarningType.LARGE_AV_SHIFT:
            type = Event.LARGE_AV_FIRST_FRAME_GAP_DETECT;
            break;
          case WarningType.LARGE_VIDEO_GAP:
            type = Event.LARGE_VIDEO_DTS_GAP_DETECT;
            break;
          case WarningType.LARGE_VIDEO_GAP_BETWEEN_CHUNK:
            type = Event.MAX_DTS_DELTA_WITH_NEXT_SEGMENT_DETECT;
            break;
        }
        if (type)
          _this.hls.emit(Event.STREAM_EXCEPTION, _objectSpread2(_objectSpread2({}, warn), {}, {
            type
          }));
        logger.warn("video exception", warn);
      });
      audioTrack.warnings.forEach(function(warn) {
        var type;
        switch (warn.type) {
          case WarningType.LARGE_AUDIO_GAP:
            type = Event.LARGE_AUDIO_DTS_GAP_DETECT;
            break;
          case WarningType.AUDIO_FILLED:
            type = Event.AUDIO_GAP_DETECT;
            break;
          case WarningType.AUDIO_DROPPED:
            type = Event.AUDIO_OVERLAP_DETECT;
            break;
        }
        if (type)
          _this.hls.emit(Event.STREAM_EXCEPTION, _objectSpread2(_objectSpread2({}, warn), {}, {
            type
          }));
        logger.warn("audio exception", warn);
      });
      videoTrack.samples.forEach(function(sample) {
        if (sample.keyframe) {
          _this.hls.emit(Event.KEYFRAME, {
            pts: sample.pts
          });
        }
      });
      metadataTrack.seiSamples.forEach(function(sei) {
        _this.hls.emit(Event.SEI, _objectSpread2(_objectSpread2({}, sei), {}, {
          originPts: sei.originPts / 90,
          sei: {
            code: sei.data.type,
            content: sei.data.payload,
            dts: sei.pts
          }
        }));
      });
    }
  }]);
  return Transmuxer2;
}();
export { Transmuxer };
